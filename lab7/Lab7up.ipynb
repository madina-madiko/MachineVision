{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return (x)*(1.0-(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [  2.20086678e-06]\n",
      "[0 1] [ 0.99723703]\n",
      "[1 0] [ 0.99619107]\n",
      "[1 1] [-0.00014896]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.8897544 ,  0.72951029, -0.34441161],\n",
      "       [-0.89515266,  0.10033336,  0.26494449],\n",
      "       [-0.29594396, -0.68769884, -0.07100018]]), array([[-0.6420993 ],\n",
      "       [-0.68110047],\n",
      "       [-0.34522147]])]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCTu/wMBOFjaAGsk7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b84181df98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.01604845]\n",
      "[0 1] [ 0.98585377]\n",
      "[1 0] [ 0.97990276]\n",
      "[1 1] [ 0.02265095]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - (x)**2\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.errors = []\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "           \n",
    "            # output layer - random((2+1, 1)) : 3 x 1\n",
    "            r = 2*np.random.random( (layers[i]+1 , layers[i+1])) -1\n",
    "            self.weights.append(r)\n",
    "        print(self.weights) \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000 ,momentum = 0.4):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        errors=[]\n",
    "        average = []\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        \n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            \n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    \n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            #self.errors.append(error**2)/(k)\n",
    "            errors.append((error**2))\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "            #plt.plot(deltas)\n",
    "            #plt.axis()\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            pr_weights_delta = []\n",
    "            pr_3_1 = np.zeros((3,1))\n",
    "            pr_3_3 = np.zeros((3,3))\n",
    "            pr_weights_delta.append(pr_3_3)\n",
    "            pr_weights_delta.append(pr_3_1)\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                delta = learning_rate * (layer.T.dot(delta)-(momentum * pr_weights_delta[i]))\n",
    "                self.weights[i] += delta\n",
    "                self.weights[i] += momentum * pr_weights_delta[i]\n",
    "                pr_weights_delta[i] = delta\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                t = np.average(errors)\n",
    "                average.append(t)\n",
    "        plt.plot(average)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()       \n",
    "        #plt.plot(delta)\n",
    "        #plt.show()\n",
    "            \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "#          print(a)\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
